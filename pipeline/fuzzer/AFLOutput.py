import os
import json
import shutil
from pipeline import utils


#####
# This class analyzes a tar file that contains fuzzing results and used to produce summary informations
# It is inherited ExpResult class to analyze the results and provides the files that ExpResults requires
#####
class AFLOutput():
    BASE_PATH = "./"              # Root fuzzer working path
    AFL_SUB_DIR = "./"           # sub root fuzzer working path (AFL++ uses: ./default)

    # default results (generated by AFL/AFL++ located in AFL_SUB_DIR)
    PLOT_DATA       = "./plot_data"      # plot data (file)
    FUZZER_STATS    = "./fuzzer_stats"   # statistics of AFL (file)
    CRASHES_DIR     = "./crashes"        # directory for crashed inputs
    QUEUE_DIR       = "./queue"          # directory for crashed inputs
    HANGS_DIR       = "./hangs"          # directory for hanging inputs

    # generated by fuzzing driver
    TOTAL_LOG           = "./total.log"         # simple log list
    NUM_INPUT_FILENAME  = "./__num__"           # number of inputs in AFL  ( not used )
    DETAILED_LOG_PATH   = "./logs"              # the folder that detailed logs are stored
    INPUT_FILE_PATH     = "./inputs"            # the folder that all inputs are stored
    FALSE_INPUT_PATH    = "./falses"            # the folder that the inputs identified as false-positive are stored
    EXPECTED_VALUE_PATH = "./expected"          # the folder that the expected values are stored (temporary)
    TOTAL_LOG_STATS     = "./stats.log"         # will be generated during analysis
    TOTAL_LOG_CONVERTED = "./total_fix.log"     # will be generated during analysis

    # internal variable
    NUM_SEED_INPUTS = 3             # number of seed inputs
    DIST_BASE_NUM   = 5000          # log/input distribution folder divider (should be the same with DIST_BASE_NUM in driver template)

    def __init__(self, _basepath, _num_seeds=3, _dist_base_num=5000, _isAFLpp=True):
        self.BASE_PATH = _basepath
        self.NUM_SEED_INPUTS = _num_seeds
        self.DIST_BASE_NUM = _dist_base_num
        if _isAFLpp is True:
            self.AFL_SUB_DIR = "./default"
        pass

    @property
    def input_dir_path(self):
        return utils.makepath(self.BASE_PATH, self.INPUT_FILE_PATH)

    @property
    def expected_dir_path(self):
        return utils.makepath(self.BASE_PATH, self.EXPECTED_VALUE_PATH)

    ###############################################################
    # get total number of inputs that tested in the AFL (__num__ file)
    ###############################################################
    def get_number_of_inputs(self):
        filepath = utils.makepath(self.BASE_PATH, self.NUM_INPUT_FILENAME)
        if os.path.exists(filepath) is False: return None

        # find tar file
        value = open(filepath, "r").read()
        value = value.strip()

        if value is None or value == "":
            return -1
        return int(value)

    ###############################################################
    # load plot data
    ###############################################################
    def load_plot(self, _reverse=False, _n_lines=0):
        '''
        return plot_data as a list of dicts:
        [
            {
            "unix_time":time (unixtime, sec), "elapsed_time":time (sec), "cycles_done":N,
            "cur_path":N, "paths_total":N, "pending_total":N, "pending_favs":N,
             "map_size":R, "unique_crashes":N, "unique_hangs":N, "max_depth":N, "execs_per_sec":R]
            },
            ...
        ]
        :param _reverse:
        :param _n_lines:
        :return:
        '''
        # load file
        filepath = utils.makepath(self.BASE_PATH, self.AFL_SUB_DIR, self.PLOT_DATA)
        if os.path.exists(filepath) is False: return None

        # get data
        data = []
        initial_time = 0
        if _reverse is False:
            with open(filepath, 'r') as read_obj:
                lines = read_obj.readlines()
                for line in lines:
                    if line == "": continue
                    if line.startswith("#") is True: continue
                    item = self.convert_plot_line(line)
                    if initial_time == 0: initial_time = item['unix_time']
                    item['elapsed_time'] = item['unix_time'] - initial_time
                    data.append(item)
        else:
            for line in utils.readline_reverse(filepath, _n_lines if _n_lines >0 else None):
                if line == "": continue
                if line.startswith("#") is True: continue

                item = self.convert_plot_line(line)
                if initial_time == 0: initial_time = item['unix_time']
                item['elapsed_time'] = item['unix_time'] - initial_time
                data.append(item)

        return data

    def load_plot_load_times(self):
        data = self.load_plot()
        if data is None: return None
        values = [item['unix_time'] for item in data]
        return values

    def load_plot_crashes(self):
        data = self.load_plot()
        if data is None: return None
        values = [item['unique_crashes'] for item in data]
        return values

    def load_plot_total_paths(self):
        data = self.load_plot()
        if data is None: return None
        values = [item['paths_total'] for item in data]
        return values

    def convert_plot_line(self, _line):
        '''
        Convert plot line into a dictionary
        :param _line:
        :return:
        '''
        colnames = ["unix_time", "cycles_done", "cur_path", "paths_total", "pending_total", "pending_favs", "map_size", "unique_crashes", "unique_hangs", "max_depth", "execs_per_sec"]
        cols = _line.split(',')
        values = []
        for idx in range(0, len(cols)):
            value = cols[idx].strip()
            if idx == 6:
                value = float(value[:-1])*100
            elif idx == 10:
                value = float(value)
            else:
                value = int(value)
            values.append(value)

        res_dct = dict(zip(colnames, values))
        return res_dct

    ###############################################################
    # load fuzzer stats
    ###############################################################
    def load_fuzzer_stats(self, _key=None):
        filepath = utils.makepath(self.BASE_PATH, self.AFL_SUB_DIR, self.FUZZER_STATS)
        if os.path.exists(filepath) is False: return None

        lines = open(filepath, "r").readlines()

        data = {}
        for line in lines:
            line = line.strip()
            if line == '': continue
            cols = line.split(":")
            key = cols[0].strip()
            value = cols[1].strip()
            if value.isnumeric(): value = int(value)
            elif self.isdouble(value): value = float(value)
            elif self.ispercent(value): value = float(value[:-1])/100.0
            data[key] = value

        if _key is not None:
            if _key in data: return data[_key]
            else:            return None
        return data

    def get_num_execs(self):
        return self.load_fuzzer_stats("execs_done")

    def isdouble(self, _string:str):
        if _string.find(".") < 0: return False

        cols = _string.split(".")
        if len(cols) > 2: return False

        if cols[0].isnumeric() and cols[1].isnumeric():
            return True
        return False

    def ispercent(self, _string:str):
        if _string.endswith("%") is False: return False
        value = _string[:-1]
        if value.isnumeric() or self.isdouble(value): return True
        return False

    ###############################################################
    # get total execution log after processing
    ###############################################################
    def make_stats_total_log(self):
        '''
        get basic statistics from the total log

        We do not consider the last execution if the execs_done (from AFL stats) is less than the actual number of executions,
        since we assume that AFL stopped during the last execution.
        :return:
        '''

        # get full path of the total log file
        filepath = utils.makepath(self.BASE_PATH, self.TOTAL_LOG)
        if os.path.exists(filepath) is False: return None, None

        # make statistics
        counts = {"all":0, "initial":0, "origin":0, "mutant":0, "comp":0}
        elapsed = {"all":-1, "initial":-1, "origin":-1, "mutant":-1, "comp":-1}
        stage = None
        flag_elapsed = False
        for record in self.iter_simple_execution_log(filepath):
            # some executions are missing. (I think this is timeout..but..we don't even have log...)
            # so I keep the last sequence ID
            counts['seq'] = record['seq']
            counts['all'] += 1

            # counts for each stage
            stage = None
            flag_elapsed = False
            if   record['initial'] is False: stage = 'initial'
            elif record['origin']  is False: stage = 'origin'
            elif record['mutant']  is False: stage = 'mutant'
            elif record['comp']    is False: stage = 'comp'
            if stage is not None:
                counts[stage] += 1
                if elapsed[stage] == -1:
                    flag_elapsed = True
                    elapsed[stage] = record['elapsed']

        # Rollback if the last execution is the non-completed execution
        #   - When a timeout of an execution happens, the execution is crashed, which will be considered in the analysis below.
        #   - When AFL finished during precondition checking, it has no _execs_done value, then we consider all the inputs.
        exec_done = self.load_fuzzer_stats(_key='execs_done')
        if exec_done is not None and exec_done < counts['seq']:
            counts['all'] -=1
            if stage is not None:
                counts[stage] -= 1
                if flag_elapsed is True: elapsed[stage] = -1

        return counts, elapsed

    def load_total_execution_log(self):
        filepath = utils.makepath(self.BASE_PATH, self.TOTAL_LOG)
        if os.path.exists(filepath) is False: return None

        data = []
        # add additional data
        for record in self.iter_simple_execution_log(filepath):
            data.append(record)
        return data

    def iter_simple_execution_log(self, _filepath):
        '''
        return an execution result as a dict
        {
            'seq': 1                # execution Order (the same to the order of input in AFL, start from 1)
            'timeID':0,             # execution ID (time ID)
            'unixtime':0,           # last executed time in seconds
            'initial': True/ False, # True if the execution passes the initialization code
            'origin': True/ False,  # True if the execution passes the original function call code
            'mutant': True/ False,  # True if the execution passes the mutated function call code
            'comp': True/ False,    # True if the comparison results of both function executions are identical
            'elapsed':0000000000    # unix time in seconds
        }
        :return:
        '''
        # load specified file
        file = open(_filepath, "r")
        line = file.readline()       # throw away the first line (title line)

        # a record for one execution
        startTime = 0
        while True:
            line = file.readline()
            line = line.strip()
            if line == '': break

            # convert column values
            cols = line.split(',')
            seqID    = int(cols[0].strip())
            timeID   = int(cols[1].strip())
            initial  = int(cols[2].strip())
            original = int(cols[3].strip()) if len(cols)>3 else 0
            mutated  = int(cols[4].strip()) if len(cols)>4 else 0
            comp     = int(cols[5].strip()) if len(cols)>5 else 0

            item = {'seq':seqID, 'timeID':timeID, 'unixtime':0,
                    'initial': False,'origin': False,'mutant': False,'comp': False}

            # update current data
            item['initial'] = True if initial == 1 else False
            item['origin'] = True if original == 1 else False
            item['mutant'] = True if mutated == 1 else False
            item['comp'] = True if comp == 1 else False

            if startTime == 0:
                item['elapsed'] = 0
                startTime = timeID
            else:
                item['elapsed'] = (timeID - startTime) / 1000000  # convert the time to seconds

            yield item.copy()

        file.close()
        return True

    def convert_total_execution_log(self):
        '''
        convert total_log into more readable results (with elapsed time)
        :return:
        '''
        # get full path of the total log file
        filepath = utils.makepath(self.BASE_PATH, self.TOTAL_LOG)
        if os.path.exists(filepath) is False: return None

        output = utils.makepath(self.BASE_PATH, self.TOTAL_LOG_CONVERTED)
        out = open(output, "w")
        out.write("SeqID,TimeID,Time,Initial,Origin,Mutant,Comparison,ElapsedTime\n")

        for record in self.iter_simple_execution_log(filepath):
            out.write("%d,%d,%d,%s,%s,%s,%s,%.3f\n" % (record['seq'], record['timeID'], record['unixtime'],
                                                       record['initial'], record['origin'],
                                                       record['mutant'], record['comp'], record['elapsed']))
            out.flush()

        out.close()
        return output

    ###############################################################
    # make and load stats of total_log
    ###############################################################
    def load_stats_total_log(self, _remake=False):
        statsfile = utils.makepath(self.BASE_PATH, self.TOTAL_LOG_STATS)
        if _remake is True or os.path.exists(statsfile) is False:
            print("\t- Not exist stats file, we are going to make it now.")
            self.make_stats_total_log()

        file_content = open(statsfile, 'r').read()
        data = json.loads(file_content)
        return data

    def store_stats_total_log(self):
        # get data
        counts, elapsed = self.make_stats_total_log()
        data = {"counts":counts, "elapsed":elapsed}

        # store data
        statsfile = utils.makepath(self.BASE_PATH, self.TOTAL_LOG_STATS)
        with open(statsfile, "w") as f:
            json.dump(data, f, indent=4)
        pass

    ###############################################################
    # find missing inputs from AFL results
    ###############################################################
    def copy_missing_inputs(self):
        # load total results
        filepath = utils.makepath(self.BASE_PATH, self.TOTAL_LOG)
        if os.path.exists(filepath) is False: return None

        # find missing inputs
        missing = []
        input_files = self.get_input_files_with_ID()
        for record in self.iter_simple_execution_log(filepath):
            if record['initial'] is False or record['origin'] is False: continue    # pass any fail in initial or origin
            if record['mutant'] is True and record['comp'] is True: continue        # pass live in all points
            if record['seq'] in input_files: continue                               # pass already listed
            missing.append(record)

        # get list of inputs
        crashed_files = self.get_crashed_input_files()
        queue_files = self.get_queue_input_files()
        hang_files = self.get_hang_input_files()

        # get crashed or queue list from total log
        data = {}
        for record in missing:
            if record['seq'] in crashed_files:
                data[record['seq']] = crashed_files[record['seq']]['filepath']
            if record['seq'] in queue_files:
                data[record['seq']] = queue_files[record['seq']]['filepath']
            if record['seq'] in hang_files:
                data[record['seq']] = hang_files[record['seq']]['filepath']

        # copy crashed inputs to inputs directory
        dest_base = os.path.join(self.BASE_PATH, self.INPUT_FILE_PATH)
        for seq in data.keys():
            src = data[seq]  #crashed_files[record['seq']]['filepath']
            fname = "%010d/%010d.inb" % (seq / self.DIST_BASE_NUM, seq)
            desc = os.path.join(dest_base, fname)
            os.makedirs(os.path.dirname(desc), exist_ok=True)
            shutil.copy(src, desc)
            print("\t Copied missing input to inputs folder: %s" % fname)
        return data

    def get_input_files_with_ID(self):
        input_files = self.get_input_files(_subName=False)

        files = {}
        for file in input_files:
            fname = os.path.basename(file)
            fname = os.path.splitext(fname)[0]
            names = fname.split("_")
            ID = int(names[0])
            files[ID] = file
        return files

    def get_crashed_input_files(self):
        crash_path = os.path.join(self.BASE_PATH, self.AFL_SUB_DIR, self.CRASHES_DIR)

        file_names = utils.get_all_files(crash_path, "id*")
        files = {}
        for file in file_names:
            attrs = self.name_to_dict(file)
            ID = int(attrs['execs'])
            files[ID] = attrs
        return files

    def get_queue_input_files(self):
        queue_path = os.path.join(self.BASE_PATH, self.AFL_SUB_DIR, self.QUEUE_DIR)

        file_names = utils.get_all_files(queue_path, "id*")
        files = {}
        for file in file_names:
            attrs = self.name_to_dict(file)
            if attrs['execs'] == "0":  # all seed inputs are "execs=0"
                ID = int(attrs['id']) + 1   # since id starts from 0
            else:
                ID = int(attrs['execs'])
            files[ID] = attrs
        return files

    def get_hang_input_files(self):
        queue_path = os.path.join(self.BASE_PATH, self.AFL_SUB_DIR, self.HANGS_DIR)

        file_names = utils.get_all_files(queue_path, "id*")
        files = {}
        for file in file_names:
            attrs = self.name_to_dict(file)
            ID = int(attrs['execs'])
            files[ID] = attrs
        return files

    def name_to_dict(self, filepath):
        attrs = dict()
        attrs['filepath'] = filepath

        # make detail attributes
        fname = os.path.basename(filepath)
        items = fname.split(",")
        idx = 0
        for item in items:
            vals = item.split(":")
            if len(vals) != 2:
                attrs[idx] = item
                idx += 1
            else:
                attrs[vals[0]] = vals[1]
        return attrs

    ###############################################################
    # get the number of seeds
    ###############################################################
    def get_number_of_seeds(self):
        return self.NUM_SEED_INPUTS

    ########################################################
    # Get path related to the dist number
    ########################################################
    def get_dist_num(self, _execID):
        return int(_execID/self.DIST_BASE_NUM)*self.DIST_BASE_NUM

    def get_input_detail_filepath(self, _execID, _revised=True):
        dist_num = self.get_dist_num(_execID)
        if _revised is False:
            filepath = utils.makepath(self.INPUT_FILE_PATH, "%d"%dist_num, "%d.inb"%_execID)
        else:
            filepath = utils.makepath(self.INPUT_FILE_PATH, "%d"%dist_num, "%d_revised1.inb"%_execID)
        return filepath

    def get_log_detail_filepath(self, _execID):
        dist_num = self.get_dist_num(_execID)
        filepath = utils.makepath(self.DETAILED_LOG_PATH, "%d"%dist_num, "%d.log"%_execID)
        return filepath

    ########################################################
    # Find issue executions in the result (available for the full parameterized results)
    ########################################################
    def find_issue_executions(self):
        crashes = self.load_plot_crashes()
        if crashes is None or len(crashes)==0: crashes = [0]
        print("\tNum of crashes: %d"%(crashes[-1]))

        counts = self.stat_execution_logs()
        print("\tstats: %s"%(str(counts)))

        execDone = self.load_fuzzer_stats(_key='execs_done')
        print("\tNumber of executions (finished): : %d"% execDone)

        log_IDs = self.get_timeIDs_from_detailed_logs()
        print("\tAll log executions: %d"% len(log_IDs))
        max_exec_ID = None
        if len(log_IDs)!=0:
            max_exec_ID = max(log_IDs)
            print("\tThe last execution ID: %d"%max_exec_ID)

        # check missing IDs in total log
        total_IDs = self.get_timeIDs_from_total_log()
        missing_IDs = log_IDs - total_IDs
        print("\tSee the missing IDs in `total.log`:")
        for ID in sorted(missing_IDs):
            if max_exec_ID is not None:
                if ID == max_exec_ID and execDone < len(log_IDs): continue
            distName = int(ID/self.DIST_BASE_NUM)*self.DIST_BASE_NUM
            print("\t\t> %s/logs/%11d/log_%d.txt"%(self.BASE_PATH, distName, ID))

            # executor = re.sub(r"(6-fuzzing[-\w]*)", '4-mutant-bins', self.BASE_PATH)+'.obj'
            # print("\t\t\t>> %s %s/inputs/%11d/input_%d.txt"%(executor, self.BASE_PATH, distName, ID))

    def stat_execution_logs(self):
        data = self.load_total_execution_log()

        counts = {"all":0, "initial":0, "origin":0, "mutant":0, "comp":0}
        for record in data:
            if record['initial'] is False: counts['initial'] += 1
            elif record['origin'] is False: counts['origin'] += 1
            elif record['mutant'] is False: counts['mutant'] += 1
            elif record['comp'] is False: counts['comp'] += 1
            counts['all'] += 1
        return counts

    def get_timeIDs_from_total_log(self):
        filepath = utils.makepath(self.BASE_PATH, self.TOTAL_LOG)
        if os.path.exists(filepath) is False: return None

        lines = open(filepath, 'r').readlines()

        timeIDs = set([])
        for line in lines:
            if line.strip() == '': break
            cols = line.split(":")
            timeID = int(cols[0])
            timeIDs.add(timeID)

        return timeIDs

    def get_timeIDs_from_detailed_logs(self):
        workpath = utils.makepath(self.BASE_PATH, self.DETAILED_LOG_PATH)

        timeIDs = set([])
        subpaths = os.listdir(workpath)
        for subpath in subpaths:
            dist_path = utils.makepath(workpath, subpath)
            if os.path.isfile(dist_path) is True: continue
            logs = os.listdir(dist_path)
            for logfile in logs:
                timeID = int(logfile[:-4])  # remove extension (from <timeID>.log to <timeID>)
                timeIDs.add(timeID)

        return timeIDs

    ########################################################
    # Do checking false positive
    ########################################################
    def get_input_files(self, _subName=False):
        # in "./inputs" with ".inb" extension.
        input_dir = utils.makepath(self.BASE_PATH, self.INPUT_FILE_PATH)
        input_files = utils.get_all_files(input_dir, "*.inb", _subName=_subName)
        return input_files

    def update_stats_false_positive(self, _false_positives):
        input_dir = utils.makepath(self.BASE_PATH, self.INPUT_FILE_PATH)
        _false_positives = [path[len(input_dir)+1:] for path in _false_positives]

        self.move_false_positives(_false_positives)

        # update stats log
        fp = open(os.path.join(self.BASE_PATH, self.TOTAL_LOG_STATS), "r")
        stats = json.load(fp)
        fp.close()

        stats["false_positives"] = _false_positives

        fp = open(os.path.join(self.BASE_PATH, self.TOTAL_LOG_STATS), "w")
        json.dump(stats, fp, indent=4)
        fp.close()
        return True

    def move_false_positives(self, false_positives):
        input_dir = utils.makepath(self.BASE_PATH, self.INPUT_FILE_PATH)
        output_dir = utils.makepath(self.BASE_PATH, self.FALSE_INPUT_PATH)

        for false_input in false_positives:
            # the path of false_positives is the relative path from "inputs" in _working_dir
            input_path = utils.makepath(input_dir, false_input)
            output_path = utils.makepath(output_dir, false_input)
            utils.prepare_directory(os.path.dirname(output_path))
            shutil.move(input_path, output_path)
        return True

    ########################################################
    # Remove duplicates
    ########################################################
    def remove_duplicate_inputs(self):
        files = self.get_input_files()
        return self.remove_duplicates(files)

    def remove_duplicates(self, files):
        count = 0
        for x in range(0, len(files)):
            for y in range(len(files)-1, x, -1):
                if self.filediff(files[x], files[y]) is True: continue
                os.remove(files[y])
                del files[y]
                count += 1
        return count

    def filediff(self, file1, file2):
        v =  open(file1, "rb")
        content1 = v.read()
        v.close()

        v =  open(file2, "rb")
        content2 = v.read()
        v.close()

        if len(content1) != len(content2): return True

        for x in range(0, len(content1)):
            if content1[x] != content2[x]: return True
        return False


if __name__ == "__main__":
    targetDirs = [
        'case_studies/ASN1/_testAFL3/6-fuzzing-exp2/test/test.mut.11.1_5_38.ROR.test',
    ]
    for targetDir in targetDirs:
        print('::: Working with %s'%targetDir)
        obj = AFLOutput(targetDir)
        obj.store_stats_total_log()
        # obj.copy_crashed_inputs()
        inputs = obj.get_input_files()
        obj.update_stats_false_positive(inputs)

        r = obj.load_stats_total_log()
        print(r)
        # obj.convert_total_execution_log_from_non_tar(targetDir + '/total.log')
        # obj.find_issue_executions()

    pass

